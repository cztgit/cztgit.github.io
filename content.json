[{"title":"Lucene","date":"2017-03-10T12:36:15.000Z","path":"2017/03/10/Lucene/","text":"Lucene 文件结构 索引：一个索引存到一个文件中 段：一个索引中可以有多个段，段和段是独立的，添加新文档会产生新的段 文档：文档是创建索引的基本单位，不同文档保存不同段 域：一个文档包含不同类型的信息，可以拆分开索引 词：词是索引的最小单位，是经过词法分析和语言处理后的数据 Lucene环境搭建 analyzers-common analyzers-smartcn lucene-core highlight queries queryparser 总共就需要六个jar包 Lucene常用功能-搜索 DirectoryReader类用来读取文件 IndexSearcher类用来搜索 Query 类用来存检索词 TopDocs 用来存查询结果 12345678910111213141516171819202122232425262728293031public class IndexSearch &#123; public static void main(String[] args) throws IOException, ParseException &#123; Directory directory = null; directory = FSDirectory.open(new File(\"D://index/test\")); DirectoryReader dReader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(dReader); //运用标准分词器 Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); //创建查询字符串 QueryParser parser = new QueryParser(Version.LUCENE_43,\"content\",analyzer); Query query = parser.parse(\"陈志涛\"); //10代表前十条 TopDocs topDocs = searcher.search(query, 10); if(topDocs != null)&#123; System.out.println(\"符合条件的文档总数：\"+topDocs.totalHits); for(int i= 0;i &lt; topDocs.scoreDocs.length;i++)&#123; Document doc = searcher.doc(topDocs.scoreDocs[i].doc); System.out.println(\"id = \"+doc.get(\"id\")); System.out.println(\"content = \"+doc.get(\"content\")); System.out.println(\"num = \"+doc.get(\"num\")); &#125; &#125; directory.close(); dReader.close(); &#125;&#125; Lucene分词器 StandarAnalyzer：标准分词器 IKAnalyzer：基于Lucene的第三方中文分词器 WhitespaceAnalyzer空格分词器 SimpleAnalyzer 简单分词器 CJKAnalyzer 二分法分词器 KeywordAnalyzer 关键词分词器 StopAnalyzer 被忽略词分词器 还有各种语言的分词器 123456789101112131415161718192021222324252627public class AnalyzerStudy &#123; private static String str = \"极客学院，Lucene案例开发\"; public static void print(Analyzer analyzer)&#123; StringReader stringreader = new StringReader(str); try &#123; TokenStream tokenStream = analyzer.tokenStream(\"\", stringreader); tokenStream.reset(); //分词结果 CharTermAttribute term = tokenStream.getAttribute(CharTermAttribute.class); System.out.println(\"分词技术\"+analyzer.getClass()); while(tokenStream.incrementToken())&#123; System.out.print(term.toString()+\"|\"); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; Analyzer analyzer = null; analyzer = new StandardAnalyzer(Version.LUCENE_43); AnalyzerStudy.print(analyzer); &#125;&#125; Query创建123456789101112131415161718192021222324252627282930package com.czt.Lu;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.Query;import org.apache.lucene.util.Version;public class QueryStudy &#123; public static void main(String[] args) throws ParseException &#123; String key = \"极客学院\"; String field = \"name\"; String [] fileds = &#123;\"name\",\"content\"&#125;; Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); Query query = null; //单个域，指定域名还有分词器 QueryParser parser = new QueryParser(Version.LUCENE_43,field,analyzer); query = parser.parse(key); System.out.println(QueryParser.class + query.toString()); //多域搜索 MultiFieldQueryParser parser1 = new MultiFieldQueryParser(Version.LUCENE_43, fileds, analyzer); query = parser1.parse(key); System.out.println(MultiFieldQueryParser.class + query.toString()); &#125; &#125; IndexSearcher常用搜索方法 Collector Filter ：筛选功能，不建议使用 Sort 在检索方法中指定排序方法 ScoreDoc 分页功能 searchAfter ：可以设置分页 网络爬虫正则表达式 元字符 出现频率 定位符 定位符 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.czt.util;import java.util.regex.Matcher;import java.io.UnsupportedEncodingException;import java.net.URLEncoder;import java.util.ArrayList;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;public class RegeUtil &#123; public static String getFirstString(String dealStr,String regeStr,int n)&#123; if(dealStr == null || regeStr == null || n &lt;1)&#123; return \"\"; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; return matcher.group().trim(); &#125; return \"\"; &#125; public static List&lt;String&gt; get(String dealStr,String regeStr,int n)&#123; List&lt;String &gt; list = new ArrayList&lt;String&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; list.add(matcher.group(n).trim()); &#125; return list; &#125; public static List&lt;String []&gt; get(String dealStr,String regeStr,int [] array,int n)&#123; List&lt;String []&gt; list = new ArrayList&lt;String[]&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; for(int i= 0;i &lt;array.length;i++)&#123; if(array[i] &lt; 1)&#123; return list; &#125; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; String []ss = new String[array.length]; for(int i = 0;i &lt; array.length;i++)&#123; ss[i] = matcher.group(array[i]).trim(); &#125; list.add(ss); &#125; return list; &#125; public static void main(String[] args) &#123; String dealStr = \"ab1234asdv\"; String regexString = \"a(.*?)a\"; System.out.println(RegeUtil.getFirstString(dealStr, regexString, 1)); &#125;&lt;a class=\"qkcontent_name\"href=\"http://www.baidu.com\"&gt;&#125; HttpClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249 /** *@Description: */ package com.jikexuyuan.crawl; import java.io.BufferedReader;import java.io.ByteArrayInputStream;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Iterator;import java.util.Map.Entry;import org.apache.commons.httpclient.Header;import org.apache.commons.httpclient.HttpClient;import org.apache.commons.httpclient.HttpMethod;import org.apache.commons.httpclient.HttpStatus;import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;import org.apache.commons.httpclient.methods.GetMethod;import org.apache.commons.httpclient.methods.PostMethod;import org.apache.log4j.Logger;import com.jikexuyuan.util.CharsetUtil; public abstract class CrawlBase &#123; private static Logger log = Logger.getLogger(CrawlBase.class); //链接源代码 private String pageSourceCode = \"\"; //返回头信息 private Header[] reponseHeaders = null; //连接超时时间 private static int connectTimeOut = 10000; //连接读取时间 private static int readTimeOut = 10000; //默认最大访问次数 private static int maxConnectTimes = 3; //网页默认编码方式 private static String charsetName = \"iso-8859-1\"; //将HttpClient委托给MultiThreadedHttpConnectionManager，支持多线程 private static MultiThreadedHttpConnectionManager httpConnectManager = new MultiThreadedHttpConnectionManager(); private static HttpClient httpClient = new HttpClient(httpConnectManager); static &#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(connectTimeOut); httpClient.getHttpConnectionManager().getParams().setSoTimeout(readTimeOut); //设置请求的编码格式 httpClient.getParams().setContentCharset(\"utf-8\"); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: GET方式请求页面 */ public boolean readPageByGet(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; GetMethod method = createGetMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: POST方式请求页面 */ public boolean readPageByPost(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; PostMethod method = createPostMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param method * @param defaultCharset * @param urlStr * @return * @Author:lulei * @Description: 执行HttpMethod，获取服务器返回的头信息和网页源代码 */ private boolean readPage(HttpMethod method, String defaultCharset, String urlStr) &#123; int n = maxConnectTimes; while (n &gt; 0) &#123; try &#123; //判断返回状态是否是200 if (httpClient.executeMethod(method) != HttpStatus.SC_OK) &#123; log.info(\"can`t connect \" + urlStr + (maxConnectTimes - n + 1)); n--; &#125; else &#123; //获取头信息 reponseHeaders = method.getRequestHeaders(); //获取服务器的输出流 InputStream inputStream = method.getResponseBodyAsStream(); BufferedReader bufferReader = new BufferedReader(new InputStreamReader(inputStream, charsetName)); StringBuffer stringBuffer = new StringBuffer(); String lineString = \"\"; while ((lineString = bufferReader.readLine()) != null) &#123; stringBuffer.append(lineString); stringBuffer.append(\"\\n\"); &#125; pageSourceCode = stringBuffer.toString(); //检测流的编码方式 InputStream in = new ByteArrayInputStream(pageSourceCode.getBytes(charsetName)); String charset = CharsetUtil.getStreamCharset(in, defaultCharset); //如果编码方式不同，则进行转码操作 if (!charsetName.toLowerCase().equals(charset.toLowerCase())) &#123; pageSourceCode = new String(pageSourceCode.getBytes(charsetName), charset); &#125; return true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error(urlStr + \"can`t connect \" + (maxConnectTimes - n + 1)); n--; &#125; &#125; return false; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建GET请求 */ @SuppressWarnings(\"rawtypes\") private GetMethod createGetMethod(String urlStr, HashMap&lt;String, String&gt; params)&#123; GetMethod method = new GetMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建POST请求 */ @SuppressWarnings(\"rawtypes\") private PostMethod createPostMethod(String urlStr, HashMap&lt;String, String&gt; params) &#123; PostMethod method = new PostMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @return String * @Author: lulei * @Description: 获取网页源代码 */ public String getPageSourceCode()&#123; return pageSourceCode; &#125; /** * @return Header[] * @Author: lulei * @Description: 获取网页返回头信息 */ public Header[] getHeader()&#123; return reponseHeaders; &#125; /** * @param timeout * @Author: lulei * @Description: 设置连接超时时间 */ public void setConnectTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(timeOut); CrawlBase.connectTimeOut = timeOut; &#125; /** * @param timeout * @Author: lulei * @Description: 设置读取超时时间 */ public void setReadTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setSoTimeout(timeOut); CrawlBase.readTimeOut = timeOut; &#125; /** * @param maxConnectTimes * @Author: lulei * @Description: 设置最大访问次数，链接失败的情况下使用 */ public static void setMaxConnectTimes(int maxConnectTimes) &#123; CrawlBase.maxConnectTimes = maxConnectTimes; &#125; /** * @param connectTimeout * @param readTimeout * @Author: lulei * @Description: 设置连接超时时间和读取超时时间 */ public void setTimeout(int connectTimeout, int readTimeout)&#123; setConnectTimeOut(connectTimeout); setReadTimeOut(readTimeout); &#125; /** * @param charsetName * @Author: lulei * @Description: 设置默认编码方式 */ public static void setCharsetName(String charsetName) &#123; CrawlBase.charsetName = charsetName; &#125; /** * @param args * @Author:lulei * @Description: */ public static void main(String[] args) &#123; // TODO Auto-generated method stub &#125;&#125; 采集过程中的工具类","tags":[{"name":"Lucene 搜索引擎","slug":"Lucene-搜索引擎","permalink":"http://yoursite.com/tags/Lucene-搜索引擎/"}]},{"title":"IDEA","date":"2017-03-10T12:36:01.000Z","path":"2017/03/10/IDEA/","text":"IDEA使用方法在Maven下面 导入项目 检查Maven是否配置ok，包括Maven的路径还有Repo路径。 修改编码 如果程序需要的jar包全都下载好，项目就完整的出来了，然后在view里面找到Tools Window-&gt;找到database还有Maven Project，在Maven Project里面，你就可以看到你所依赖的jar包。 然后添加下Facets，添加Spring。 添加数据库，最后要在Maven Project里面先clean一下，在install一下。","tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"技术路线","date":"2017-02-27T01:23:37.000Z","path":"2017/02/27/技术路线/","text":"雄关漫道真如铁，而今迈步从头越 这学期主要修完3门课： 论文读写。 Lucene开发，极客学院的视频。 深度学习视频。 加油吧~~~","tags":[{"name":"坚持","slug":"坚持","permalink":"http://yoursite.com/tags/坚持/"}]},{"title":"新学期","date":"2017-02-20T05:27:18.000Z","path":"2017/02/20/新学期/","text":"新的学期，每天跑步，每天有新的计划，就好了。 有很多事情不知道怎么安排，但是还是不断的去挑战。 论文。 刷题。 机器学习。 搜索引擎。 网络爬虫。 不断的坚持吧。","tags":[{"name":"专注 坚持","slug":"专注-坚持","permalink":"http://yoursite.com/tags/专注-坚持/"}]},{"title":"IDEA","date":"2017-02-13T05:24:09.000Z","path":"2017/02/13/IDEA使用/","text":"IDEA使用方法在Maven下面 导入项目 检查Maven是否配置ok，包括Maven的路径还有Repo路径。 修改编码 如果程序需要的jar包全都下载好，项目就完整的出来了，然后在view里面找到Tools Window-&gt;找到database还有Maven Project，在Maven Project里面，你就可以看到你所依赖的jar包。 然后添加下Facets，添加Spring。 添加数据库，最后要在Maven Project里面先clean一下，在install一下。","tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"传道授业","date":"2017-02-13T05:24:09.000Z","path":"2017/02/13/传道受业/","text":"今天给实验室的师弟师妹，讲了讲JavaWeb项目。 为了后续找工作能快点入门。 回头想想，研一时候，研一没有师兄师姐带，都是自己搞，有时候卡住一个问题，就在网上查很多天。 团队是云南项目组的师弟师妹，研一来了，就跟我干项目，我觉得她们跟我干。 后面什么事情，我能帮忙的，必须帮忙。 不怕狼一样的需求，就怕猪一样的团队。 我很希望有个好团队，只要团队和谐融洽，项目做起来是一种享受的感觉，再加班都不累。 痛并快乐着。 讲了一上午，我觉得老师这个活挺累。 非常想听到互动，而不是干干巴巴的讲，这让我回想起来我的学习生涯，我一直都是课堂上的活跃分子，老师讲的每个点，我都会大声说：对。有时候搞的同学们都哈哈大笑。 后面还有许多事需要再考虑，希望团队里每个成员都能快速成长，能找到好工作，这就是我的想法。","tags":[]},{"title":"Lucene学习","date":"2017-02-13T05:24:09.000Z","path":"2017/02/13/Lucene学习/","text":"Lucene 文件结构 索引：一个索引存到一个文件中 段：一个索引中可以有多个段，段和段是独立的，添加新文档会产生新的段 文档：文档是创建索引的基本单位，不同文档保存不同段 域：一个文档包含不同类型的信息，可以拆分开索引 词：词是索引的最小单位，是经过词法分析和语言处理后的数据 Lucene环境搭建 analyzers-common analyzers-smartcn lucene-core highlight queries queryparser 总共就需要六个jar包 Lucene常用功能-搜索 DirectoryReader类用来读取文件 IndexSearcher类用来搜索 Query 类用来存检索词 TopDocs 用来存查询结果 12345678910111213141516171819202122232425262728293031public class IndexSearch &#123; public static void main(String[] args) throws IOException, ParseException &#123; Directory directory = null; directory = FSDirectory.open(new File(\"D://index/test\")); DirectoryReader dReader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(dReader); //运用标准分词器 Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); //创建查询字符串 QueryParser parser = new QueryParser(Version.LUCENE_43,\"content\",analyzer); Query query = parser.parse(\"陈志涛\"); //10代表前十条 TopDocs topDocs = searcher.search(query, 10); if(topDocs != null)&#123; System.out.println(\"符合条件的文档总数：\"+topDocs.totalHits); for(int i= 0;i &lt; topDocs.scoreDocs.length;i++)&#123; Document doc = searcher.doc(topDocs.scoreDocs[i].doc); System.out.println(\"id = \"+doc.get(\"id\")); System.out.println(\"content = \"+doc.get(\"content\")); System.out.println(\"num = \"+doc.get(\"num\")); &#125; &#125; directory.close(); dReader.close(); &#125;&#125; Lucene分词器 StandarAnalyzer：标准分词器 IKAnalyzer：基于Lucene的第三方中文分词器 WhitespaceAnalyzer空格分词器 SimpleAnalyzer 简单分词器 CJKAnalyzer 二分法分词器 KeywordAnalyzer 关键词分词器 StopAnalyzer 被忽略词分词器 还有各种语言的分词器 123456789101112131415161718192021222324252627public class AnalyzerStudy &#123; private static String str = \"极客学院，Lucene案例开发\"; public static void print(Analyzer analyzer)&#123; StringReader stringreader = new StringReader(str); try &#123; TokenStream tokenStream = analyzer.tokenStream(\"\", stringreader); tokenStream.reset(); //分词结果 CharTermAttribute term = tokenStream.getAttribute(CharTermAttribute.class); System.out.println(\"分词技术\"+analyzer.getClass()); while(tokenStream.incrementToken())&#123; System.out.print(term.toString()+\"|\"); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; Analyzer analyzer = null; analyzer = new StandardAnalyzer(Version.LUCENE_43); AnalyzerStudy.print(analyzer); &#125;&#125; Query创建123456789101112131415161718192021222324252627282930package com.czt.Lu;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.Query;import org.apache.lucene.util.Version;public class QueryStudy &#123; public static void main(String[] args) throws ParseException &#123; String key = \"极客学院\"; String field = \"name\"; String [] fileds = &#123;\"name\",\"content\"&#125;; Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); Query query = null; //单个域，指定域名还有分词器 QueryParser parser = new QueryParser(Version.LUCENE_43,field,analyzer); query = parser.parse(key); System.out.println(QueryParser.class + query.toString()); //多域搜索 MultiFieldQueryParser parser1 = new MultiFieldQueryParser(Version.LUCENE_43, fileds, analyzer); query = parser1.parse(key); System.out.println(MultiFieldQueryParser.class + query.toString()); &#125; &#125; IndexSearcher常用搜索方法 Collector Filter ：筛选功能，不建议使用 Sort 在检索方法中指定排序方法 ScoreDoc 分页功能 searchAfter ：可以设置分页 网络爬虫正则表达式 元字符 出现频率 定位符 定位符 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.czt.util;import java.util.regex.Matcher;import java.io.UnsupportedEncodingException;import java.net.URLEncoder;import java.util.ArrayList;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;public class RegeUtil &#123; public static String getFirstString(String dealStr,String regeStr,int n)&#123; if(dealStr == null || regeStr == null || n &lt;1)&#123; return \"\"; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; return matcher.group().trim(); &#125; return \"\"; &#125; public static List&lt;String&gt; get(String dealStr,String regeStr,int n)&#123; List&lt;String &gt; list = new ArrayList&lt;String&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; list.add(matcher.group(n).trim()); &#125; return list; &#125; public static List&lt;String []&gt; get(String dealStr,String regeStr,int [] array,int n)&#123; List&lt;String []&gt; list = new ArrayList&lt;String[]&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; for(int i= 0;i &lt;array.length;i++)&#123; if(array[i] &lt; 1)&#123; return list; &#125; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; String []ss = new String[array.length]; for(int i = 0;i &lt; array.length;i++)&#123; ss[i] = matcher.group(array[i]).trim(); &#125; list.add(ss); &#125; return list; &#125; public static void main(String[] args) &#123; String dealStr = \"ab1234asdv\"; String regexString = \"a(.*?)a\"; System.out.println(RegeUtil.getFirstString(dealStr, regexString, 1)); &#125;&lt;a class=\"qkcontent_name\"href=\"http://www.baidu.com\"&gt;&#125; HttpClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249 /** *@Description: */ package com.jikexuyuan.crawl; import java.io.BufferedReader;import java.io.ByteArrayInputStream;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Iterator;import java.util.Map.Entry;import org.apache.commons.httpclient.Header;import org.apache.commons.httpclient.HttpClient;import org.apache.commons.httpclient.HttpMethod;import org.apache.commons.httpclient.HttpStatus;import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;import org.apache.commons.httpclient.methods.GetMethod;import org.apache.commons.httpclient.methods.PostMethod;import org.apache.log4j.Logger;import com.jikexuyuan.util.CharsetUtil; public abstract class CrawlBase &#123; private static Logger log = Logger.getLogger(CrawlBase.class); //链接源代码 private String pageSourceCode = \"\"; //返回头信息 private Header[] reponseHeaders = null; //连接超时时间 private static int connectTimeOut = 10000; //连接读取时间 private static int readTimeOut = 10000; //默认最大访问次数 private static int maxConnectTimes = 3; //网页默认编码方式 private static String charsetName = \"iso-8859-1\"; //将HttpClient委托给MultiThreadedHttpConnectionManager，支持多线程 private static MultiThreadedHttpConnectionManager httpConnectManager = new MultiThreadedHttpConnectionManager(); private static HttpClient httpClient = new HttpClient(httpConnectManager); static &#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(connectTimeOut); httpClient.getHttpConnectionManager().getParams().setSoTimeout(readTimeOut); //设置请求的编码格式 httpClient.getParams().setContentCharset(\"utf-8\"); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: GET方式请求页面 */ public boolean readPageByGet(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; GetMethod method = createGetMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: POST方式请求页面 */ public boolean readPageByPost(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; PostMethod method = createPostMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param method * @param defaultCharset * @param urlStr * @return * @Author:lulei * @Description: 执行HttpMethod，获取服务器返回的头信息和网页源代码 */ private boolean readPage(HttpMethod method, String defaultCharset, String urlStr) &#123; int n = maxConnectTimes; while (n &gt; 0) &#123; try &#123; //判断返回状态是否是200 if (httpClient.executeMethod(method) != HttpStatus.SC_OK) &#123; log.info(\"can`t connect \" + urlStr + (maxConnectTimes - n + 1)); n--; &#125; else &#123; //获取头信息 reponseHeaders = method.getRequestHeaders(); //获取服务器的输出流 InputStream inputStream = method.getResponseBodyAsStream(); BufferedReader bufferReader = new BufferedReader(new InputStreamReader(inputStream, charsetName)); StringBuffer stringBuffer = new StringBuffer(); String lineString = \"\"; while ((lineString = bufferReader.readLine()) != null) &#123; stringBuffer.append(lineString); stringBuffer.append(\"\\n\"); &#125; pageSourceCode = stringBuffer.toString(); //检测流的编码方式 InputStream in = new ByteArrayInputStream(pageSourceCode.getBytes(charsetName)); String charset = CharsetUtil.getStreamCharset(in, defaultCharset); //如果编码方式不同，则进行转码操作 if (!charsetName.toLowerCase().equals(charset.toLowerCase())) &#123; pageSourceCode = new String(pageSourceCode.getBytes(charsetName), charset); &#125; return true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error(urlStr + \"can`t connect \" + (maxConnectTimes - n + 1)); n--; &#125; &#125; return false; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建GET请求 */ @SuppressWarnings(\"rawtypes\") private GetMethod createGetMethod(String urlStr, HashMap&lt;String, String&gt; params)&#123; GetMethod method = new GetMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建POST请求 */ @SuppressWarnings(\"rawtypes\") private PostMethod createPostMethod(String urlStr, HashMap&lt;String, String&gt; params) &#123; PostMethod method = new PostMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @return String * @Author: lulei * @Description: 获取网页源代码 */ public String getPageSourceCode()&#123; return pageSourceCode; &#125; /** * @return Header[] * @Author: lulei * @Description: 获取网页返回头信息 */ public Header[] getHeader()&#123; return reponseHeaders; &#125; /** * @param timeout * @Author: lulei * @Description: 设置连接超时时间 */ public void setConnectTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(timeOut); CrawlBase.connectTimeOut = timeOut; &#125; /** * @param timeout * @Author: lulei * @Description: 设置读取超时时间 */ public void setReadTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setSoTimeout(timeOut); CrawlBase.readTimeOut = timeOut; &#125; /** * @param maxConnectTimes * @Author: lulei * @Description: 设置最大访问次数，链接失败的情况下使用 */ public static void setMaxConnectTimes(int maxConnectTimes) &#123; CrawlBase.maxConnectTimes = maxConnectTimes; &#125; /** * @param connectTimeout * @param readTimeout * @Author: lulei * @Description: 设置连接超时时间和读取超时时间 */ public void setTimeout(int connectTimeout, int readTimeout)&#123; setConnectTimeOut(connectTimeout); setReadTimeOut(readTimeout); &#125; /** * @param charsetName * @Author: lulei * @Description: 设置默认编码方式 */ public static void setCharsetName(String charsetName) &#123; CrawlBase.charsetName = charsetName; &#125; /** * @param args * @Author:lulei * @Description: */ public static void main(String[] args) &#123; // TODO Auto-generated method stub &#125;&#125; 采集过程中的工具类","tags":[{"name":"坚持 努力","slug":"坚持-努力","permalink":"http://yoursite.com/tags/坚持-努力/"}]},{"title":"新的征途","date":"2017-02-12T13:01:31.000Z","path":"2017/02/12/新的征途/","text":"2017年开学早早的就来了，路过了北京，见了些朋友，还都不错，过着自己的生活，并没有太多变化。很高兴的是，自己搭建了一个hexo博客，也不知道为什么。之前大学一直用csdn，而如今也转向自己的博客。2017一切都是新的，一切都要努力的去前行。祝好。","tags":[]}]