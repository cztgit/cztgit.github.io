[{"title":"mysql存储引擎","date":"2017-08-10T08:24:09.000Z","path":"2017/08/10/存储引擎/","text":"上天让你放弃和等待，是为了给你最好的 数据库存储引擎对于初学者来说我们通常不关注存储引擎，但是 MySQL 提供了多个存储引擎，包括处理事务安全表的引擎和处理非事务安全表的引擎。在 MySQL 中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。 存储引擎简介MySQL中的数据用各种不同的技术存储在文件(或者内存)中。这些技术中的每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力。通过选择不同的技术，你能够获得额外的速度或者功能，从而改善你的应用的整体功能。 存储引擎说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。 例如，如果你在研究大量的临时数据，你也许需要使用内存存储引擎。内存存储引擎能够在内存中存储所有的表格数据。又或者，你也许需要一个支持事务处理的数据库(以确保事务处理不成功时数据的回退能力)。 InnoDBInnoDB是一个健壮的事务型存储引擎，这种存储引擎已经被很多互联网公司使用，为用户操作非常大的数据存储提供了一个强大的解决方案。我的电脑上安装的 MySQL 5.6.13 版，InnoDB就是作为默认的存储引擎。InnoDB还引入了行级锁定和外键约束，在以下场合下，使用InnoDB是最理想的选择： 更新密集的表。InnoDB存储引擎特别适合处理多重并发的更新请求。 事务。InnoDB存储引擎是支持事务的标准MySQL存储引擎。 自动灾难恢复。与其它存储引擎不同，InnoDB表能够自动从灾难中恢复。 外键约束。MySQL支持外键的存储引擎只有InnoDB。 支持自动增加列AUTO_INCREMENT属性。 从5.7开始innodb存储引擎成为默认的存储引擎。 一般来说，如果需要事务支持，并且有较高的并发读取频率，InnoDB是不错的选择。 MyISAMMyISAM表是独立于操作系统的，这说明可以轻松地将其从Windows服务器移植到Linux服务器；每当我们建立一个MyISAM引擎的表时，就会在本地磁盘上建立三个文件，文件名就是表名。例如，我建立了一个MyISAM引擎的tb_Demo表，那么就会生成以下三个文件： tb_demo.frm，存储表定义。 tb_demo.MYD，存储数据。 tb_demo.MYI，存储索引。 MyISAM表无法处理事务，这就意味着有事务处理需求的表，不能使用MyISAM存储引擎。MyISAM存储引擎特别适合在以下几种情况下使用： 选择密集型的表。MyISAM存储引擎在筛选大量数据时非常迅速，这是它最突出的优点。 插入密集型的表。MyISAM的并发插入特性允许同时选择和插入数据。例如：MyISAM存储引擎很适合管理邮件或Web服务器日志数据。 MRG_MYISAMMRG_MyISAM存储引擎是一组MyISAM表的组合，老版本叫 MERGE 其实是一回事儿，这些MyISAM表结构必须完全相同，尽管其使用不如其它引擎突出，但是在某些情况下非常有用。说白了，Merge表就是几个相同MyISAM表的聚合器；Merge表中并没有数据，对Merge类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行操作。 Merge存储引擎的使用场景。对于服务器日志这种信息，一般常用的存储策略是将数据分成很多表，每个名称与特定的时间端相关。例如：可以用12个相同的表来存储服务器日志数据，每个表用对应各个月份的名字来命名。当有必要基于所有12个日志表的数据来生成报表，这意味着需要编写并更新多表查询，以反映这些表中的信息。与其编写这些可能出现错误的查询，不如将这些表合并起来使用一条查询，之后再删除Merge表，而不影响原来的数据，删除Merge表只是删除Merge表的定义，对内部的表没有任何影响。 ENGINE=MERGE，指明使用MERGE引擎，其实是跟MRG_MyISAM一回事儿，也是对的，在MySQL 5.7已经看不到MERGE了。 UNION=(t1, t2)，指明了MERGE表中挂接了些哪表，可以通过alter table的方式修改UNION的值，以实现增删MERGE表子表的功能。比如： 1alter table tb_merge engine=merge union(tb_log1) insert_method=last; INSERT_METHOD=LAST，INSERT_METHOD指明插入方式，取值可以是：0 不允许插入；FIRST 插入到UNION中的第一个表； LAST 插入到UNION中的最后一个表。 MERGE表及构成MERGE数据表结构的各成员数据表必须具有完全一样的结构。每一个成员数据表的数据列必须按照同样的顺序定义同样的名字和类型，索引也必须按照同样的顺序和同样的方式定义。 MEMORY使用MySQL Memory存储引擎的出发点是速度。为得到最快的响应时间，采用的逻辑存储介质是系统内存。虽然在内存中存储表数据确实会提供很高的性能，但当mysqld守护进程崩溃时，所有的Memory数据都会丢失。获得速度的同时也带来了一些缺陷。它要求存储在Memory数据表里的数据使用的是长度不变的格式，这意味着不能使用BLOB和TEXT这样的长度可变的数据类型，VARCHAR是一种长度可变的类型，但因为它在MySQL内部当做长度固定不变的CHAR类型，所以可以使用。 一般在以下几种情况下使用Memory存储引擎： 目标数据较小，而且被非常频繁地访问。在内存中存放数据，所以会造成内存的使用，可以通过参数max_heap_table_size控制Memory表的大小，设置此参数，就可以限制Memory表的最大大小。 如果数据是临时的，而且要求必须立即可用，那么就可以存放在内存表中。 存储在Memory表中的数据如果突然丢失，不会对应用服务产生实质的负面影响。 Memory同时支持散列索引和B树索引。B树索引的优于散列索引的是，可以使用部分查询和通配查询，也可以使用&lt;、&gt;和&gt;=等操作符方便数据挖掘。散列索引进行“相等比较”非常快，但是对“范围比较”的速度就慢多了，因此散列索引值适合使用在=和&lt;&gt;的操作符中，不适合在&lt;或&gt;操作符中，也同样不适合用在order by子句中。 CSVCSV 存储引擎是基于 CSV 格式文件存储数据。 CSV 存储引擎因为自身文件格式的原因，所有列必须强制指定 NOT NULL 。 CSV 引擎也不支持索引，不支持分区。 CSV 存储引擎也会包含一个存储表结构的 .frm 文件，还会创建一个 .csv 存储数据的文件，还会创建一个同名的元信息文件，该文件的扩展名为 .CSM ，用来保存表的状态及表中保存的数据量。 每个数据行占用一个文本行。 因为 csv 文件本身就可以被Office等软件直接编辑，保不齐就有不按规则出牌的情况，如果出现csv 文件中的内容损坏了的情况，也可以使用 CHECK TABLE 或者 REPAIR TABLE 命令检查和修复 ARCHIVEArchive是归档的意思，在归档之后很多的高级功能就不再支持了，仅仅支持最基本的插入和查询两种功能。在MySQL 5.5版以前，Archive是不支持索引，但是在MySQL 5.5以后的版本中就开始支持索引了。Archive拥有很好的压缩机制，它使用zlib压缩库，在记录被请求时会实时压缩，所以它经常被用来当做仓库使用。 BLACKHOLE黑洞存储引擎，所有插入的数据并不会保存，BLACKHOLE 引擎表永远保持为空，写入的任何数据都会消失， PERFORMANCE_SCHEMA主要用于收集数据库服务器性能参数。MySQL用户是不能创建存储引擎为PERFORMANCE_SCHEMA的表，一般用于记录binlog做复制的中继。在这里有官方的一些介绍MySQL Performance Schema FEDERATED主要用于访问其它远程MySQL服务器一个代理，它通过创建一个到远程MySQL服务器的客户端连接，并将查询传输到远程服务器执行，而后完成数据存取；在MariaDB的上实现是FederatedX 其他这里列举一些其它数据库提供的存储引擎，OQGraph、SphinxSE、TokuDB、Cassandra、CONNECT、SQUENCE。提供的名字仅供参考。 常用引擎对比不同存储引起都有各自的特点，为适应不同的需求，需要选择不同的存储引擎，所以首先考虑这些存储引擎各自的功能和兼容。 特性 InnoDB MyISAM MEMORY ARCHIVE 存储限制(Storage limits) 64TB No YES No 支持事物(Transactions) Yes No No No 锁机制(Locking granularity) 行锁 表锁 表锁 行锁 B树索引(B-tree indexes) Yes Yes Yes No T树索引(T-tree indexes) No No No No 哈希索引(Hash indexes) Yes No Yes No 全文索引(Full-text indexes) Yes Yes No No 集群索引(Clustered indexes) Yes No No No 数据缓存(Data caches) Yes No N/A No 索引缓存(Index caches) Yes Yes N/A No 数据可压缩(Compressed data) Yes Yes No Yes 加密传输(Encrypted data[1]) Yes Yes Yes Yes 集群数据库支持(Cluster databases support) No No No No 复制支持(Replication support[2]) Yes No No Yes 外键支持(Foreign key support) Yes No No No 存储空间消耗(Storage Cost) 高 低 N/A 非常低 内存消耗(Memory Cost) 高 低 N/A 低 数据字典更新(Update statistics for data dictionary) Yes Yes Yes Yes 备份/时间点恢复(backup/point-in-time recovery[3]) Yes Yes Yes Yes 多版本并发控制(Multi-Version Concurrency Control/MVCC) Yes No No No 批量数据写入效率(Bulk insert speed) 慢 快 快 非常快 地理信息数据类型(Geospatial datatype support) Yes Yes No Yes 地理信息索引(Geospatial indexing support[4]) Yes Yes No Yes 在服务器中实现（通过加密功能）。在其他表空间加密数据在MySQL 5.7或更高版本兼容。 在服务中实现的，而不是在存储引擎中实现的。 在服务中实现的，而不是在存储引擎中实现的。 地理位置索引，InnoDB支持可mysql5.7.5或更高版本兼容 查看存储引擎使用“SHOW VARIABLES LIKE ‘%storage_engine%’;” 命令在mysql系统变量搜索磨人设置的存储引擎，输入语句如下： 1234567891011mysql&gt; SHOW VARIABLES LIKE '%storage_engine%';+----------------------------------+---------+| Variable_name | Value ||----------------------------------+---------|| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || disabled_storage_engines | || internal_tmp_disk_storage_engine | InnoDB |+----------------------------------+---------+4 rows in setTime: 0.005s 使用“SHOW ENGINES;”命令显示安装以后可用的所有的支持的存储引擎和默认引擎，后面带上 \\G 可以列表输出结果，你可以尝试一下如“SHOW ENGINES\\G;”。 1234567891011121314151617mysql&gt; SHOW ENGINES;+--------------------+---------+--------------------------------------+-------------+--------+-----------+| Engine | Support | Comment | Transactions| XA | Savepoints||--------------------+---------+--------------------------------------+-------------+--------+-----------|| InnoDB | DEFAULT | Supports transactions, | YES | YES | YES || | | row-level locking, and foreign keys | | | || MRG_MYISAM | YES | Collection of identical MyISAM tables| NO | NO | NO || MEMORY | YES | Hash based, stored in memory, useful | NO | NO | NO || | | for temporary tables | | | || BLACKHOLE | YES | /dev/null storage engine (anything | NO | NO | NO || | | you write to it disappears) | | | || MyISAM | YES | MyISAM storage engine | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || ARCHIVE | YES | Archive storage engine | NO | NO | NO || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || FEDERATED | NO | Federated MySQL storage engine | &lt;null&gt; | &lt;null&gt; | &lt;null&gt; |+--------------------+---------+--------------------------------------+-------------+--------+-----------+ 由上面命令输出，可见当前系统的默认数据表类型是InnoDB。当然，我们可以通过修改数据库配置文件中的选项，设定默认表类型。 设置存储引擎对上面数据库存储引擎有所了解之后，你可以在my.cnf 配置文件中设置你需要的存储引擎，这个参数放在 [mysqld] 这个字段下面的 default_storage_engine 参数值，例如下面配置的片段 12[mysqld]default_storage_engine=CSV 在创建表的时候，对表设置存储引擎，例如： 123456CREATE TABLE `user` ( `id` int(100) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(32) NOT NULL DEFAULT '' COMMENT '姓名', `mobile` varchar(20) NOT NULL DEFAULT '' COMMENT '手机', PRIMARY KEY (`id`))ENGINE=InnoDB; 在创建用户表 user 的时候，SQL语句最后 ENGINE=InnoDB 就是设置这张表存储引擎为 InnoDB。 如何选择合适的存储引擎提供几个选择标准，然后按照标准，选择对应的存储引擎即可，也可以根据常用引擎对比来选择你使用的存储引擎。使用哪种引擎需要根据需求灵活选择，一个数据库中多个表可以使用不同的引擎以满足各种性能和实际需求。使用合适的存储引擎，将会提高整个数据库的性能。 是否需要支持事务； 是否需要使用热备； 崩溃恢复，能否接受崩溃； 是否需要外键支持； 存储的限制； 对索引和缓存的支持； ​","tags":[]},{"title":"心情","date":"2017-08-03T08:24:09.000Z","path":"2017/08/03/无题/","text":"上天让你放弃和等待，是为了给你最好的 最近压力较大，不知道论文能不能搞的出来，内心有些焦虑，有些恐惧。 不知道未来的路是什么样的， 不敢去想未来的路， 真的不敢。 而我给的就是顺其自然吧。 好好做好当下。 最近自己也有很多问题， 没有聚焦的去搞一件事情， 心里有些躁动， 争取做好每一天， 不浪费时间， 对自己有信心， 没有问题！ ​","tags":[]},{"title":"ConcurrentMap","date":"2017-08-03T08:24:09.000Z","path":"2017/08/03/ConcurrentMap/","text":"简介ConcurrentHashMap 是 util.concurrent 包的重要成员。本文将结合 Java 内存模型，分析 JDK 源代码，探索 ConcurrentHashMap 高并发的具体实现机制。 由于 ConcurrentHashMap 的源代码实现依赖于 Java 内存模型，所以阅读本文需要读者了解 Java 内存模型。同时，ConcurrentHashMap 的源代码会涉及到散列算法和链表数据结构，所以，读者需要对散列算法和基于链表的数据结构有所了解。 Java 内存模型由于 ConcurrentHashMap 是建立在 Java 内存模型基础上的，为了更好的理解 ConcurrentHashMap，让我们首先来了解一下 Java 的内存模型。 Java 语言的内存模型由一些规则组成，这些规则确定线程对内存的访问如何排序以及何时可以确保它们对线程是可见的。下面我们将分别介绍 Java 内存模型的重排序，内存可见性和 happens-before 关系。 重排序内存模型描述了程序的可能行为。具体的编译器实现可以产生任意它喜欢的代码 – 只要所有执行这些代码产生的结果，能够和内存模型预测的结果保持一致。这为编译器实现者提供了很大的自由，包括操作的重排序。 编译器生成指令的次序，可以不同于源代码所暗示的“显然”版本。重排序后的指令，对于优化执行以及成熟的全局寄存器分配算法的使用，都是大有脾益的，它使得程序在计算性能上有了很大的提升。 重排序类型包括： 编译器生成指令的次序，可以不同于源代码所暗示的“显然”版本。 处理器可以乱序或者并行的执行指令。 缓存会改变写入提交到主内存的变量的次序。 内存可见性由于现代可共享内存的多处理器架构可能导致一个线程无法马上（甚至永远）看到另一个线程操作产生的结果。所以 Java 内存模型规定了 JVM 的一种最小保证：什么时候写入一个变量对其他线程可见。 在现代可共享内存的多处理器体系结构中每个处理器都有自己的缓存，并周期性的与主内存协调一致。假设线程 A 写入一个变量值 V，随后另一个线程 B 读取变量 V 的值，在下列情况下，线程 B 读取的值可能不是线程 A 写入的最新值： 执行线程 A 的处理器把变量 V 缓存到寄存器中。 执行线程 A 的处理器把变量 V 缓存到自己的缓存中，但还没有同步刷新到主内存中去。 执行线程 B 的处理器的缓存中有变量 V 的旧值。 Happens-before 关系happens-before 关系保证：如果线程 A 与线程 B 满足 happens-before 关系，则线程 A 执行动作的结果对于线程 B 是可见的。如果两个操作未按 happens-before 排序，JVM 将可以对他们任意重排序。 下面介绍几个与理解 ConcurrentHashMap 有关的 happens-before 关系法则： 程序次序法则：如果在程序中，所有动作 A 出现在动作 B 之前，则线程中的每动作 A 都 happens-before 于该线程中的每一个动作 B。 监视器锁法则：对一个监视器的解锁 happens-before 于每个后续对同一监视器的加锁。 Volatile 变量法则：对 Volatile 域的写入操作 happens-before 于每个后续对同一 Volatile 的读操作。 传递性：如果 A happens-before 于 B，且 B happens-before C，则 A happens-before C。 ConcurrentHashMap 的结构分析为了更好的理解 ConcurrentHashMap 高并发的具体实现，让我们先探索它的结构模型。 ConcurrentHashMap 类中包含两个静态内部类 HashEntry 和 Segment。HashEntry 用来封装映射表的键 / 值对；Segment 用来充当锁的角色，每个 Segment 对象守护整个散列映射表的若干个桶。每个桶是由若干个 HashEntry 对象链接起来的链表。一个 ConcurrentHashMap 实例中包含由若干个 Segment 对象组成的数组。 HashEntry 类HashEntry 用来封装散列映射表中的键值对。在 HashEntry 类中，key，hash 和 next 域都被声明为 final 型，value 域被声明为 volatile 型。 清单 1.HashEntry 类的定义12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final K key; // 声明 key 为 final 型 final int hash; // 声明 hash 值为 final 型 volatile V value; // 声明 value 为 volatile 型 final HashEntry&lt;K,V&gt; next; // 声明 next 为 final 型 HashEntry(K key, int hash, HashEntry&lt;K,V&gt; next, V value) &#123; this.key = key; this.hash = hash; this.next = next; this.value = value; &#125; &#125; 在 ConcurrentHashMap 中，在散列时如果产生“碰撞”，将采用“分离链接法”来处理“碰撞”：把“碰撞”的 HashEntry 对象链接成一个链表。由于 HashEntry 的 next 域为 final 型，所以新节点只能在链表的表头处插入。 下图是在一个空桶中依次插入 A，B，C 三个 HashEntry 对象后的结构图： 图 1. 插入三个节点后桶的结构示意图： 注意：由于只能在表头插入，所以链表中节点的顺序和插入的顺序相反。 避免热点域 在 ConcurrentHashMap中，每一个 Segment 对象都有一个 count 对象来表示本 Segment 中包含的 HashEntry 对象的个数。这样当需要更新计数器时，不用锁定整个 ConcurrentHashMap。 Segment 类Segment 类继承于 ReentrantLock 类，从而使得 Segment 对象能充当锁的角色。每个 Segment 对象用来守护其（成员对象 table 中）包含的若干个桶。 table 是一个由 HashEntry 对象组成的数组。table 数组的每一个数组成员就是散列映射表的一个桶。 count 变量是一个计数器，它表示每个 Segment 对象管理的 table 数组（若干个 HashEntry 组成的链表）包含的 HashEntry 对象的个数。每一个 Segment 对象都有一个 count 对象来表示本 Segment 中包含的 HashEntry 对象的总数。注意，之所以在每个 Segment 对象中包含一个计数器，而不是在 ConcurrentHashMap 中使用全局的计数器，是为了避免出现“热点域”而影响 ConcurrentHashMap 的并发性。 清单 2.Segment 类的定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; /** * 在本 segment 范围内，包含的 HashEntry 元素的个数 * 该变量被声明为 volatile 型 */ transient volatile int count; /** * table 被更新的次数 */ transient int modCount; /** * 当 table 中包含的 HashEntry 元素的个数超过本变量值时，触发 table 的再散列 */ transient int threshold; /** * table 是由 HashEntry 对象组成的数组 * 如果散列时发生碰撞，碰撞的 HashEntry 对象就以链表的形式链接成一个链表 * table 数组的数组成员代表散列映射表的一个桶 * 每个 table 守护整个 ConcurrentHashMap 包含桶总数的一部分 * 如果并发级别为 16，table 则守护 ConcurrentHashMap 包含的桶总数的 1/16 */ transient volatile HashEntry&lt;K,V&gt;[] table; /** * 装载因子 */ final float loadFactor; Segment(int initialCapacity, float lf) &#123; loadFactor = lf; setTable(HashEntry.&lt;K,V&gt;newArray(initialCapacity)); &#125; /** * 设置 table 引用到这个新生成的 HashEntry 数组 * 只能在持有锁或构造函数中调用本方法 */ void setTable(HashEntry&lt;K,V&gt;[] newTable) &#123; // 计算临界阀值为新数组的长度与装载因子的乘积 threshold = (int)(newTable.length * loadFactor); table = newTable; &#125; /** * 根据 key 的散列值，找到 table 中对应的那个桶（table 数组的某个数组成员） */ HashEntry&lt;K,V&gt; getFirst(int hash) &#123; HashEntry&lt;K,V&gt;[] tab = table; // 把散列值与 table 数组长度减 1 的值相“与”，// 得到散列值对应的 table 数组的下标 // 然后返回 table 数组中此下标对应的 HashEntry 元素 return tab[hash &amp; (tab.length - 1)]; &#125; &#125; 下图是依次插入 ABC 三个 HashEntry 节点后，Segment 的结构示意图。 图 2. 插入三个节点后 Segment 的结构示意图： ConcurrentHashMap 类ConcurrentHashMap 在默认并发级别会创建包含 16 个 Segment 对象的数组。每个 Segment 的成员对象 table 包含若干个散列表的桶。每个桶是由 HashEntry 链接起来的一个链表。如果键能均匀散列，每个 Segment 大约守护整个散列表中桶总数的 1/16。 清单 3.ConcurrentHashMap 类的定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable &#123; /** * 散列映射表的默认初始容量为 16，即初始默认为 16 个桶 * 在构造函数中没有指定这个参数时，使用本参数 */ static final int DEFAULT_INITIAL_CAPACITY= 16; /** * 散列映射表的默认装载因子为 0.75，该值是 table 中包含的 HashEntry 元素的个数与* table 数组长度的比值 * 当 table 中包含的 HashEntry 元素的个数超过了 table 数组的长度与装载因子的乘积时，* 将触发 再散列 * 在构造函数中没有指定这个参数时，使用本参数 */ static final float DEFAULT_LOAD_FACTOR= 0.75f; /** * 散列表的默认并发级别为 16。该值表示当前更新线程的估计数 * 在构造函数中没有指定这个参数时，使用本参数 */ static final int DEFAULT_CONCURRENCY_LEVEL= 16; /** * segments 的掩码值 * key 的散列码的高位用来选择具体的 segment */ final int segmentMask; /** * 偏移量 */ final int segmentShift; /** * 由 Segment 对象组成的数组 */ final Segment&lt;K,V&gt;[] segments; /** * 创建一个带有指定初始容量、加载因子和并发级别的新的空映射。 */ public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if(!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if(concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // 寻找最佳匹配参数（不小于给定参数的最接近的 2 次幂） int sshift = 0; int ssize = 1; while(ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; segmentShift = 32 - sshift; // 偏移量值 segmentMask = ssize - 1; // 掩码值 this.segments = Segment.newArray(ssize); // 创建数组 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if(c * ssize &lt; initialCapacity) ++c; int cap = 1; while(cap &lt; c) cap &lt;&lt;= 1; // 依次遍历每个数组元素 for(int i = 0; i &lt; this.segments.length; ++i) // 初始化每个数组元素引用的 Segment 对象this.segments[i] = new Segment&lt;K,V&gt;(cap, loadFactor); &#125; /** * 创建一个带有默认初始容量 (16)、默认加载因子 (0.75) 和 默认并发级别 (16) * 的空散列映射表。 */ public ConcurrentHashMap() &#123; // 使用三个默认参数，调用上面重载的构造函数来创建空散列映射表this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); &#125; 下面是 ConcurrentHashMap 的结构示意图。 图 3.ConcurrentHashMap 的结构示意图： 用分离锁实现多个线程间的并发写操作在 ConcurrentHashMap 中，线程对映射表做读操作时，一般情况下不需要加锁就可以完成，对容器做结构性修改的操作才需要加锁。下面以 put 操作为例说明对 ConcurrentHashMap 做结构性修改的过程。 首先，根据 key 计算出对应的 hash 值： 清单 4.Put 方法的实现1234567public V put(K key, V value) &#123; if (value == null) //ConcurrentHashMap 中不允许用 null 作为映射值 throw new NullPointerException(); int hash = hash(key.hashCode()); // 计算键对应的散列码 // 根据散列码找到对应的 Segment return segmentFor(hash).put(key, hash, value, false); &#125; 然后，根据 hash 值找到对应的Segment 对象： 清单 5.根据 hash 值找到对应的 Segment12345678910/** * 使用 key 的散列码来得到 segments 数组中对应的 Segment */ final Segment&lt;K,V&gt; segmentFor(int hash) &#123; // 将散列值右移 segmentShift 个位，并在高位填充 0 // 然后把得到的值与 segmentMask 相“与”// 从而得到 hash 值对应的 segments 数组的下标值// 最后根据下标值返回散列码对应的 Segment 对象 return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask]; &#125; 最后，在这个 Segment 中执行具体的 put 操作： 清单 6.在 Segment 中执行具体的 put 操作12345678910111213141516171819202122232425262728293031323334353637V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; lock(); // 加锁，这里是锁定某个 Segment 对象而非整个 ConcurrentHashMap try &#123; int c = count; if (c++ &gt; threshold) // 如果超过再散列的阈值 rehash(); // 执行再散列，table 数组的长度将扩充一倍 HashEntry&lt;K,V&gt;[] tab = table; // 把散列码值与 table 数组的长度减 1 的值相“与” // 得到该散列码对应的 table 数组的下标值 int index = hash &amp; (tab.length - 1); // 找到散列码对应的具体的那个桶 HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue; if (e != null) &#123; // 如果键 / 值对以经存在 oldValue = e.value; if (!onlyIfAbsent) e.value = value; // 设置 value 值 &#125; else &#123; // 键 / 值对不存在 oldValue = null; ++modCount; // 要添加新节点到链表中，所以 modCont 要加 1 // 创建新节点，并添加到链表的头部 tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); count = c; // 写 count 变量 &#125; return oldValue; &#125; finally &#123; unlock(); // 解锁 &#125; &#125; 注意：这里的加锁操作是针对（键的 hash 值对应的）某个具体的 Segment，锁定的是该 Segment 而不是整个 ConcurrentHashMap。因为插入键 / 值对操作只是在这个 Segment 包含的某个桶中完成，不需要锁定整个ConcurrentHashMap。此时，其他写线程对另外 15 个Segment 的加锁并不会因为当前线程对这个 Segment 的加锁而阻塞。同时，所有读线程几乎不会因本线程的加锁而阻塞（除非读线程刚好读到这个 Segment 中某个 HashEntry 的 value 域的值为 null，此时需要加锁后重新读取该值）。 相比较于 HashTable 和由同步包装器包装的 HashMap``每次只能有一个线程执行读或写操作，ConcurrentHashMap 在并发访问性能上有了质的提高。在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设置为 16），及任意数量线程的读操作。 用 HashEntery 对象的不变性来降低读操作对加锁的需求在代码清单“HashEntry 类的定义”中我们可以看到，HashEntry 中的 key，hash，next 都声明为 final 型。这意味着，不能把节点添加到链接的中间和尾部，也不能在链接的中间和尾部删除节点。这个特性可以保证：在访问某个节点时，这个节点之后的链接不会被改变。这个特性可以大大降低处理链表时的复杂性。 同时，HashEntry 类的 value 域被声明为 Volatile 型，Java 的内存模型可以保证：某个写线程对 value 域的写入马上可以被后续的某个读线程“看”到。在 ConcurrentHashMap 中，不允许用 unll 作为键和值，当读线程读到某个 HashEntry 的 value 域的值为 null 时，便知道产生了冲突——发生了重排序现象，需要加锁后重新读入这个 value 值。这些特性互相配合，使得读线程即使在不加锁状态下，也能正确访问 ConcurrentHashMap。 下面我们分别来分析线程写入的两种情形：对散列表做非结构性修改的操作和对散列表做结构性修改的操作。 非结构性修改操作只是更改某个 HashEntry 的 value 域的值。由于对 Volatile 变量的写入操作将与随后对这个变量的读操作进行同步。当一个写线程修改了某个 HashEntry 的 value 域后，另一个读线程读这个值域，Java 内存模型能够保证读线程读取的一定是更新后的值。所以，写线程对链表的非结构性修改能够被后续不加锁的读线程“看到”。 对 ConcurrentHashMap 做结构性修改，实质上是对某个桶指向的链表做结构性修改。如果能够确保：在读线程遍历一个链表期间，写线程对这个链表所做的结构性修改不影响读线程继续正常遍历这个链表。那么读 / 写线程之间就可以安全并发访问这个 ConcurrentHashMap。 结构性修改操作包括 put，remove，clear。下面我们分别分析这三个操作。 clear 操作只是把 ConcurrentHashMap 中所有的桶“置空”，每个桶之前引用的链表依然存在，只是桶不再引用到这些链表（所有链表的结构并没有被修改）。正在遍历某个链表的读线程依然可以正常执行对该链表的遍历。 从上面的代码清单“在 Segment 中执行具体的 put 操作”中，我们可以看出：put 操作如果需要插入一个新节点到链表中时 , 会在链表头部插入这个新节点。此时，链表中的原有节点的链接并没有被修改。也就是说：插入新健 / 值对到链表中的操作不会影响读线程正常遍历这个链表。 下面来分析 remove 操作，先让我们来看看 remove 操作的源代码实现。 清单 7.remove 操作123456789101112131415161718192021222324252627282930313233343536V remove(Object key, int hash, Object value) &#123; lock(); // 加锁 try&#123; int c = count - 1; HashEntry&lt;K,V&gt;[] tab = table; // 根据散列码找到 table 的下标值 int index = hash &amp; (tab.length - 1); // 找到散列码对应的那个桶 HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while(e != null&amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue = null; if(e != null) &#123; V v = e.value; if(value == null|| value.equals(v)) &#123; // 找到要删除的节点 oldValue = v; ++modCount; // 所有处于待删除节点之后的节点原样保留在链表中 // 所有处于待删除节点之前的节点被克隆到新链表中 HashEntry&lt;K,V&gt; newFirst = e.next;// 待删节点的后继结点 for(HashEntry&lt;K,V&gt; p = first; p != e; p = p.next) newFirst = new HashEntry&lt;K,V&gt;(p.key, p.hash, newFirst, p.value); // 把桶链接到新的头结点 // 新的头结点是原链表中，删除节点之前的那个节点 tab[index] = newFirst; count = c; // 写 count 变量 &#125; &#125; return oldValue; &#125; finally&#123; unlock(); // 解锁 &#125; &#125; 和 get 操作一样，首先根据散列码找到具体的链表；然后遍历这个链表找到要删除的节点；最后把待删除节点之后的所有节点原样保留在新链表中，把待删除节点之前的每个节点克隆到新链表中。下面通过图例来说明 remove 操作。假设写线程执行 remove 操作，要删除链表的 C 节点，另一个读线程同时正在遍历这个链表。 图 4. 执行删除之前的原链表： 图 5. 执行删除之后的新链表 从上图可以看出，删除节点 C 之后的所有节点原样保留到新链表中；删除节点 C 之前的每个节点被克隆到新链表中，注意：它们在新链表中的链接顺序被反转了。 在执行 remove 操作时，原始链表并没有被修改，也就是说：读线程不会受同时执行 remove 操作的并发写线程的干扰。 综合上面的分析我们可以看出，写线程对某个链表的结构性修改不会影响其他的并发读线程对这个链表的遍历访问。 用 Volatile 变量协调读写线程间的内存可见性由于内存可见性问题，未正确同步的情况下，写线程写入的值可能并不为后续的读线程可见。 下面以写线程 M 和读线程 N 来说明 ConcurrentHashMap 如何协调读 / 写线程间的内存可见性问题。 图 6. 协调读 - 写线程间的内存可见性的示意图： 假设线程 M 在写入了 volatile 型变量 count 后，线程 N 读取了这个 volatile 型变量 count。 根据 happens-before 关系法则中的程序次序法则，A appens-before 于 B，C happens-before D。 根据 Volatile 变量法则，B happens-before C。 根据传递性，连接上面三个 happens-before 关系得到：A appens-before 于 B； B appens-before C；C happens-before D。也就是说：写线程 M 对链表做的结构性修改，在读线程 N 读取了同一个 volatile 变量后，对线程 N 也是可见的了。 虽然线程 N 是在未加锁的情况下访问链表。Java 的内存模型可以保证：只要之前对链表做结构性修改操作的写线程 M 在退出写方法前写 volatile 型变量 count，读线程 N 在读取这个 volatile 型变量 count 后，就一定能“看到”这些修改。 ConcurrentHashMap 中，每个 Segment 都有一个变量 count。它用来统计 Segment 中的 HashEntry 的个数。这个变量被声明为 volatile。 清单 8.Count 变量的声明所有不加锁读方法，在进入读方法时，首先都会去读这个 count 变量。比如下面的 get 方法： 1transient volatile int count; 清单 9.get 操作12345678910111213141516V get(Object key, int hash) &#123; if(count != 0) &#123; // 首先读 count 变量 HashEntry&lt;K,V&gt; e = getFirst(hash); while(e != null) &#123; if(e.hash == hash &amp;&amp; key.equals(e.key)) &#123; V v = e.value; if(v != null) return v; // 如果读到 value 域为 null，说明发生了重排序，加锁后重新读取 return readValueUnderLock(e); &#125; e = e.next; &#125; &#125; return null; &#125; 在 ConcurrentHashMap 中，所有执行写操作的方法（put, remove, clear），在对链表做结构性修改之后，在退出写方法前都会去写这个 count 变量。所有未加锁的读操作（get, contains, containsKey）在读方法中，都会首先去读取这个 count 变量。 根据 Java 内存模型，对 同一个 volatile 变量的写 / 读操作可以确保：写线程写入的值，能够被之后未加锁的读线程“看到”。 这个特性和前面介绍的 HashEntry 对象的不变性相结合，使得在 ConcurrentHashMap 中，读线程在读取散列表时，基本不需要加锁就能成功获得需要的值。这两个特性相配合，不仅减少了请求同一个锁的频率（读操作一般不需要加锁就能够成功获得值），也减少了持有同一个锁的时间（只有读到 value 域的值为 null 时 , 读线程才需要加锁后重读）。 ConcurrentHashMap 实现高并发的总结基于通常情形而优化在实际的应用中，散列表一般的应用场景是：除了少数插入操作和删除操作外，绝大多数都是读取操作，而且读操作在大多数时候都是成功的。正是基于这个前提，ConcurrentHashMap 针对读操作做了大量的优化。通过 HashEntry 对象的不变性和用 volatile 型变量协调线程间的内存可见性，使得 大多数时候，读操作不需要加锁就可以正确获得值。这个特性使得 ConcurrentHashMap 的并发性能在分离锁的基础上又有了近一步的提高。 总结ConcurrentHashMap 是一个并发散列映射表的实现，它允许完全并发的读取，并且支持给定数量的并发更新。相比于 HashTable 和用同步包装器包装的 HashMap（Collections.synchronizedMap(new HashMap())），ConcurrentHashMap 拥有更高的并发性。在 HashTable 和由同步包装器包装的 HashMap 中，使用一个全局的锁来同步不同线程间的并发访问。同一时间点，只能有一个线程持有锁，也就是说在同一时间点，只能有一个线程能访问容器。这虽然保证多线程间的安全并发访问，但同时也导致对容器的访问变成``*串行化*``的了。 在使用锁来协调多线程间并发访问的模式下，减小对锁的竞争可以有效提高并发性。有两种方式可以减小对锁的竞争： 减小请求 同一个锁的 频率。 减少持有锁的 时间。 ConcurrentHashMap 的高并发性主要来自于三个方面： 用分离锁实现多个线程间的更深层次的共享访问。 用 HashEntery 对象的不变性来降低执行读操作的线程在遍历链表期间对加锁的需求。 通过对同一个 Volatile 变量的写 / 读访问，协调不同线程间读 / 写操作的内存可见性。 使用分离锁，减小了请求 同一个锁的频率。 通过 HashEntery 对象的不变性及对同一个 Volatile 变量的读 / 写来协调内存可见性，使得 读操作大多数时候不需要加锁就能成功获取到需要的值。由于散列映射表在实际应用中大多数操作都是成功的 读操作，所以 2 和 3 既可以减少请求同一个锁的频率，也可以有效减少持有锁的时间。 通过减小请求同一个锁的频率和尽量减少持有锁的时间 ，使得 ConcurrentHashMap 的并发性相对于 HashTable 和用同步包装器包装的 HashMap有了质的提高。","tags":[]},{"title":"堆分析","date":"2017-07-28T08:24:09.000Z","path":"2017/07/28/JVM运行机制/","text":"JVM启动流程 PC寄存器 每个线程拥有一个PC寄存器 在线程创建时创建 指向下一条指令的地址 执行本地方法时，PC的值为undefined 方法区 保存类的信息 类的常量池 字段，方法信息 方法字节码 通常和永久区Perm关联在一起。 Java堆 new出来的对象，都在这里。 Java栈 线程私有 帧中保存方法的局部变量、操作数栈、常量池指针 每一次方法调用创建一个帧，并压栈 局部变量表 参数和局部变量 函数调用组成 帧栈 操作数栈 堆栈交互 内存模型 可见性 一个线程修改了变量，其他线程可以立即知道。 保证可见性的方法 volatile synchronized（unlock 之前，写变量值回主存） final(一旦初始化完成，其他线程就可见) 有序性 线程内，有序 线程外，无序。 ​","tags":[]},{"title":"性能监控工具","date":"2017-07-23T05:24:09.000Z","path":"2017/07/23/性能监控工具/","text":"top 各种cpu情况 vmstat 统计CPu内存，swap情况还有IO等情况。 pidstat 监控进程 pidstat -t 显示线程 pidstat -p -t -d 显示io情况 pslist 命令行工具 自耦东华数据收集 显示Java程序运行情况 JAVA 自带工具 jps: 列出java进程，类似ps命令 -q 进程id号 -m 传递给main函数的参数 -l 显示路径 -v 显示传第给jvm的参数 jdb.exe jhat.exe jinfo.exe 查看java 应用程序的扩展参数 -flag: 打印指定JVM的参数 jmap.exe 生成java应用程序的堆快照和对象的统计信息 jmap -histo 2972 &gt; C:\\s.txt jmap -dump:format=b,file=c:\\heap.hprof 2972 输出堆信息 jstack.exe 打印线程dump jstack 120 &gt;&gt; c:\\a.txt JConsole 图形化监控工具 可以查看Java应用程序运行概况。 Visual VM 图形诊断和性能监控可视化工具。 问题排查 问题是程序卡死，没有预期输出 jps 查看程序的pid top 查看cpu的占用情况，看到有个进程利用cpu达到100% pidstat -p 3455 1 3 -u -t 查看线程的使用情况 jstack 3455 ，找到线程，可以定位具体的在哪个位置。","tags":[{"name":"top vmstat pslist","slug":"top-vmstat-pslist","permalink":"http://yoursite.com/tags/top-vmstat-pslist/"}]},{"title":"堆分析","date":"2017-07-19T05:24:09.000Z","path":"2017/07/19/堆分析/","text":"堆溢出 PermGen space 永久区溢出 增大 永久区，运行回收 Exception in thread “main” java.lang.OutOfMemeryError: Unable to create new native thread 栈空间不足，需要解决减少堆空间，减少栈空间。 直接内存溢出 ByteBuffer.allocateDirect() 无法从操作系统获得足够的空间。 at sun.misc.Unsafe.allocateMemory MAT使用基础 支配者被回收，被支配对象都被回收。 MAT可以显示一个对象引用的对象，显示引用这个对象的对象。 ​","tags":[{"name":"MAT Visual VM","slug":"MAT-Visual-VM","permalink":"http://yoursite.com/tags/MAT-Visual-VM/"}]},{"title":"GC参数","date":"2017-07-10T12:36:15.000Z","path":"2017/07/10/GC收集器算法/","text":"GC 参数 串行收集器 -XX:+UseSerialGC : 在新生代和老年带使用串行收集器。 并行收集 -XX:+UseParNewGC 并行收集器，只影响新生代收集。 复制算法，多线程，需要多线程支持。 -XX:ParallelGCThreads 限制线程数量。 Parallel收集器 新生代复制算法，老年代标记-压缩算法 -XX:+UseParallelGC : 使用Parallel收集器+老年带串行 -XX:+UseParallelOldGC: 使用Parallel收集器+并行老年带。 CMS收集器 Concurrent Mark Sweep 并发标记清除算法。 标记-清除算法。 并发阶段降低吞吐量。 老年代收集器。 和应用程序一起执行。 -XX:+UseConcMarkSweepGC 打开CMS收集器 新生代使用并行收集器，老年带使用CMS+串行收集器 标记压缩算法，不会产生碎片。CMS是并发执行，所以他不能用标记压缩算法，得用标记清理，标记压缩要移动内存位置。 -XX:+ UserCMSCompactAtFullCollection 在一次full GC后，进行一次整理，整理过程独占，会引起停顿时间变长。 -XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理。 -XX:ParallelCMSThreads 设定CMS的线程数量。 减轻GC压力 性能测试工具 JMeter JVM尽量会维持在很小的堆上来进行操作，这样GC次数会增多，所以可以观察下这个项目最小的堆内存是多少，然后给设置一个最小堆，这样就会减少堆的GC次数。 如果没有指定，默认是串行收集器 jdk6和jdk7，对性能都会不一样。7好一点。 ​ ​","tags":[{"name":"GC JVM","slug":"GC-JVM","permalink":"http://yoursite.com/tags/GC-JVM/"}]},{"title":"类装载器","date":"2017-07-05T05:24:09.000Z","path":"2017/07/05/类装载器/","text":"链接 验证 ： 保证Class 流的格式正确的。文件格式验证，元数据认证，字节码认证，符号引用验证。 准备：public static int v = 1； 在准备阶段，v = 0；的。如果public static final int v= 1；在准备阶段就是1. 解析： 符号引用替换直接引用。 符号引用 引用对象不一定被加载。 直接引用 指针活地址偏移量。 引用对象一定在内存。 初始化 执行类构造器 static 变量赋值语句 static{} 语句 类装载器ClassLoader BootStrap 启动 rt.jar Extension 扩展 lib/ext/*.jar App Custom 自底向上检测是否加载。加载是由顶向下。","tags":[{"name":"BootStrap  Extension","slug":"BootStrap-Extension","permalink":"http://yoursite.com/tags/BootStrap-Extension/"}]},{"title":"Lucene","date":"2017-03-10T12:36:15.000Z","path":"2017/03/10/Lucene/","text":"Lucene 文件结构 索引：一个索引存到一个文件中 段：一个索引中可以有多个段，段和段是独立的，添加新文档会产生新的段 文档：文档是创建索引的基本单位，不同文档保存不同段 域：一个文档包含不同类型的信息，可以拆分开索引 词：词是索引的最小单位，是经过词法分析和语言处理后的数据 Lucene环境搭建 analyzers-common analyzers-smartcn lucene-core highlight queries queryparser 总共就需要六个jar包 Lucene常用功能-搜索 DirectoryReader类用来读取文件 IndexSearcher类用来搜索 Query 类用来存检索词 TopDocs 用来存查询结果 12345678910111213141516171819202122232425262728293031public class IndexSearch &#123; public static void main(String[] args) throws IOException, ParseException &#123; Directory directory = null; directory = FSDirectory.open(new File(\"D://index/test\")); DirectoryReader dReader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(dReader); //运用标准分词器 Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); //创建查询字符串 QueryParser parser = new QueryParser(Version.LUCENE_43,\"content\",analyzer); Query query = parser.parse(\"陈志涛\"); //10代表前十条 TopDocs topDocs = searcher.search(query, 10); if(topDocs != null)&#123; System.out.println(\"符合条件的文档总数：\"+topDocs.totalHits); for(int i= 0;i &lt; topDocs.scoreDocs.length;i++)&#123; Document doc = searcher.doc(topDocs.scoreDocs[i].doc); System.out.println(\"id = \"+doc.get(\"id\")); System.out.println(\"content = \"+doc.get(\"content\")); System.out.println(\"num = \"+doc.get(\"num\")); &#125; &#125; directory.close(); dReader.close(); &#125;&#125; Lucene分词器 StandarAnalyzer：标准分词器 IKAnalyzer：基于Lucene的第三方中文分词器 WhitespaceAnalyzer空格分词器 SimpleAnalyzer 简单分词器 CJKAnalyzer 二分法分词器 KeywordAnalyzer 关键词分词器 StopAnalyzer 被忽略词分词器 还有各种语言的分词器 123456789101112131415161718192021222324252627public class AnalyzerStudy &#123; private static String str = \"极客学院，Lucene案例开发\"; public static void print(Analyzer analyzer)&#123; StringReader stringreader = new StringReader(str); try &#123; TokenStream tokenStream = analyzer.tokenStream(\"\", stringreader); tokenStream.reset(); //分词结果 CharTermAttribute term = tokenStream.getAttribute(CharTermAttribute.class); System.out.println(\"分词技术\"+analyzer.getClass()); while(tokenStream.incrementToken())&#123; System.out.print(term.toString()+\"|\"); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; Analyzer analyzer = null; analyzer = new StandardAnalyzer(Version.LUCENE_43); AnalyzerStudy.print(analyzer); &#125;&#125; Query创建123456789101112131415161718192021222324252627282930package com.czt.Lu;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.Query;import org.apache.lucene.util.Version;public class QueryStudy &#123; public static void main(String[] args) throws ParseException &#123; String key = \"极客学院\"; String field = \"name\"; String [] fileds = &#123;\"name\",\"content\"&#125;; Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43); Query query = null; //单个域，指定域名还有分词器 QueryParser parser = new QueryParser(Version.LUCENE_43,field,analyzer); query = parser.parse(key); System.out.println(QueryParser.class + query.toString()); //多域搜索 MultiFieldQueryParser parser1 = new MultiFieldQueryParser(Version.LUCENE_43, fileds, analyzer); query = parser1.parse(key); System.out.println(MultiFieldQueryParser.class + query.toString()); &#125; &#125; IndexSearcher常用搜索方法 Collector Filter ：筛选功能，不建议使用 Sort 在检索方法中指定排序方法 ScoreDoc 分页功能 searchAfter ：可以设置分页 网络爬虫正则表达式 元字符 出现频率 定位符 定位符 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.czt.util;import java.util.regex.Matcher;import java.io.UnsupportedEncodingException;import java.net.URLEncoder;import java.util.ArrayList;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;public class RegeUtil &#123; public static String getFirstString(String dealStr,String regeStr,int n)&#123; if(dealStr == null || regeStr == null || n &lt;1)&#123; return \"\"; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; return matcher.group().trim(); &#125; return \"\"; &#125; public static List&lt;String&gt; get(String dealStr,String regeStr,int n)&#123; List&lt;String &gt; list = new ArrayList&lt;String&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; list.add(matcher.group(n).trim()); &#125; return list; &#125; public static List&lt;String []&gt; get(String dealStr,String regeStr,int [] array,int n)&#123; List&lt;String []&gt; list = new ArrayList&lt;String[]&gt;(); if(dealStr == null || regeStr == null || n &lt;1)&#123; return list; &#125; for(int i= 0;i &lt;array.length;i++)&#123; if(array[i] &lt; 1)&#123; return list; &#125; &#125; Pattern pattern = Pattern.compile(regeStr, Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(dealStr); while(matcher.find())&#123; String []ss = new String[array.length]; for(int i = 0;i &lt; array.length;i++)&#123; ss[i] = matcher.group(array[i]).trim(); &#125; list.add(ss); &#125; return list; &#125; public static void main(String[] args) &#123; String dealStr = \"ab1234asdv\"; String regexString = \"a(.*?)a\"; System.out.println(RegeUtil.getFirstString(dealStr, regexString, 1)); &#125;&lt;a class=\"qkcontent_name\"href=\"http://www.baidu.com\"&gt;&#125; HttpClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249 /** *@Description: */ package com.jikexuyuan.crawl; import java.io.BufferedReader;import java.io.ByteArrayInputStream;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Iterator;import java.util.Map.Entry;import org.apache.commons.httpclient.Header;import org.apache.commons.httpclient.HttpClient;import org.apache.commons.httpclient.HttpMethod;import org.apache.commons.httpclient.HttpStatus;import org.apache.commons.httpclient.MultiThreadedHttpConnectionManager;import org.apache.commons.httpclient.methods.GetMethod;import org.apache.commons.httpclient.methods.PostMethod;import org.apache.log4j.Logger;import com.jikexuyuan.util.CharsetUtil; public abstract class CrawlBase &#123; private static Logger log = Logger.getLogger(CrawlBase.class); //链接源代码 private String pageSourceCode = \"\"; //返回头信息 private Header[] reponseHeaders = null; //连接超时时间 private static int connectTimeOut = 10000; //连接读取时间 private static int readTimeOut = 10000; //默认最大访问次数 private static int maxConnectTimes = 3; //网页默认编码方式 private static String charsetName = \"iso-8859-1\"; //将HttpClient委托给MultiThreadedHttpConnectionManager，支持多线程 private static MultiThreadedHttpConnectionManager httpConnectManager = new MultiThreadedHttpConnectionManager(); private static HttpClient httpClient = new HttpClient(httpConnectManager); static &#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(connectTimeOut); httpClient.getHttpConnectionManager().getParams().setSoTimeout(readTimeOut); //设置请求的编码格式 httpClient.getParams().setContentCharset(\"utf-8\"); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: GET方式请求页面 */ public boolean readPageByGet(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; GetMethod method = createGetMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param urlStr * @param params * @param charsetName * @return * @Author:lulei * @Description: POST方式请求页面 */ public boolean readPageByPost(String urlStr, HashMap&lt;String, String&gt; params, String charsetName) &#123; PostMethod method = createPostMethod(urlStr, params); return readPage(method, charsetName, urlStr); &#125; /** * @param method * @param defaultCharset * @param urlStr * @return * @Author:lulei * @Description: 执行HttpMethod，获取服务器返回的头信息和网页源代码 */ private boolean readPage(HttpMethod method, String defaultCharset, String urlStr) &#123; int n = maxConnectTimes; while (n &gt; 0) &#123; try &#123; //判断返回状态是否是200 if (httpClient.executeMethod(method) != HttpStatus.SC_OK) &#123; log.info(\"can`t connect \" + urlStr + (maxConnectTimes - n + 1)); n--; &#125; else &#123; //获取头信息 reponseHeaders = method.getRequestHeaders(); //获取服务器的输出流 InputStream inputStream = method.getResponseBodyAsStream(); BufferedReader bufferReader = new BufferedReader(new InputStreamReader(inputStream, charsetName)); StringBuffer stringBuffer = new StringBuffer(); String lineString = \"\"; while ((lineString = bufferReader.readLine()) != null) &#123; stringBuffer.append(lineString); stringBuffer.append(\"\\n\"); &#125; pageSourceCode = stringBuffer.toString(); //检测流的编码方式 InputStream in = new ByteArrayInputStream(pageSourceCode.getBytes(charsetName)); String charset = CharsetUtil.getStreamCharset(in, defaultCharset); //如果编码方式不同，则进行转码操作 if (!charsetName.toLowerCase().equals(charset.toLowerCase())) &#123; pageSourceCode = new String(pageSourceCode.getBytes(charsetName), charset); &#125; return true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error(urlStr + \"can`t connect \" + (maxConnectTimes - n + 1)); n--; &#125; &#125; return false; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建GET请求 */ @SuppressWarnings(\"rawtypes\") private GetMethod createGetMethod(String urlStr, HashMap&lt;String, String&gt; params)&#123; GetMethod method = new GetMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @param urlStr * @param params 请求头信息 * @return * @Author:lulei * @Description: 创建POST请求 */ @SuppressWarnings(\"rawtypes\") private PostMethod createPostMethod(String urlStr, HashMap&lt;String, String&gt; params) &#123; PostMethod method = new PostMethod(urlStr); if (params == null) &#123; return method; &#125; Iterator&lt;Entry&lt;String, String&gt;&gt; itor = params.entrySet().iterator(); while (itor.hasNext()) &#123; Entry entry = (Entry) itor.next(); String key = (String) entry.getKey(); String val = (String) entry.getValue(); method.setRequestHeader(key, val); &#125; return method; &#125; /** * @return String * @Author: lulei * @Description: 获取网页源代码 */ public String getPageSourceCode()&#123; return pageSourceCode; &#125; /** * @return Header[] * @Author: lulei * @Description: 获取网页返回头信息 */ public Header[] getHeader()&#123; return reponseHeaders; &#125; /** * @param timeout * @Author: lulei * @Description: 设置连接超时时间 */ public void setConnectTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(timeOut); CrawlBase.connectTimeOut = timeOut; &#125; /** * @param timeout * @Author: lulei * @Description: 设置读取超时时间 */ public void setReadTimeOut(int timeOut)&#123; httpClient.getHttpConnectionManager().getParams().setSoTimeout(timeOut); CrawlBase.readTimeOut = timeOut; &#125; /** * @param maxConnectTimes * @Author: lulei * @Description: 设置最大访问次数，链接失败的情况下使用 */ public static void setMaxConnectTimes(int maxConnectTimes) &#123; CrawlBase.maxConnectTimes = maxConnectTimes; &#125; /** * @param connectTimeout * @param readTimeout * @Author: lulei * @Description: 设置连接超时时间和读取超时时间 */ public void setTimeout(int connectTimeout, int readTimeout)&#123; setConnectTimeOut(connectTimeout); setReadTimeOut(readTimeout); &#125; /** * @param charsetName * @Author: lulei * @Description: 设置默认编码方式 */ public static void setCharsetName(String charsetName) &#123; CrawlBase.charsetName = charsetName; &#125; /** * @param args * @Author:lulei * @Description: */ public static void main(String[] args) &#123; // TODO Auto-generated method stub &#125;&#125; 采集过程中的工具类","tags":[{"name":"Lucene 搜索引擎","slug":"Lucene-搜索引擎","permalink":"http://yoursite.com/tags/Lucene-搜索引擎/"}]},{"title":"IDEA","date":"2017-03-10T12:36:01.000Z","path":"2017/03/10/IDEA/","text":"IDEA使用方法在Maven下面 导入项目 检查Maven是否配置ok，包括Maven的路径还有Repo路径。 修改编码 如果程序需要的jar包全都下载好，项目就完整的出来了，然后在view里面找到Tools Window-&gt;找到database还有Maven Project，在Maven Project里面，你就可以看到你所依赖的jar包。 然后添加下Facets，添加Spring。 添加数据库，最后要在Maven Project里面先clean一下，在install一下。","tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"技术路线","date":"2017-02-27T01:23:37.000Z","path":"2017/02/27/技术路线/","text":"雄关漫道真如铁，而今迈步从头越 这学期主要修完3门课： 论文读写。 Lucene开发，极客学院的视频。 深度学习视频。 加油吧~~~","tags":[{"name":"坚持","slug":"坚持","permalink":"http://yoursite.com/tags/坚持/"}]},{"title":"新学期","date":"2017-02-20T05:27:18.000Z","path":"2017/02/20/新学期/","text":"新的学期，每天跑步，每天有新的计划，就好了。 有很多事情不知道怎么安排，但是还是不断的去挑战。 论文。 刷题。 机器学习。 搜索引擎。 网络爬虫。 不断的坚持吧。","tags":[{"name":"专注 坚持","slug":"专注-坚持","permalink":"http://yoursite.com/tags/专注-坚持/"}]},{"title":"传道授业","date":"2017-02-13T05:24:09.000Z","path":"2017/02/13/传道受业/","text":"今天给实验室的师弟师妹，讲了讲JavaWeb项目。 为了后续找工作能快点入门。 回头想想，研一时候，研一没有师兄师姐带，都是自己搞，有时候卡住一个问题，就在网上查很多天。 团队是云南项目组的师弟师妹，研一来了，就跟我干项目，我觉得她们跟我干。 后面什么事情，我能帮忙的，必须帮忙。 不怕狼一样的需求，就怕猪一样的团队。 我很希望有个好团队，只要团队和谐融洽，项目做起来是一种享受的感觉，再加班都不累。 痛并快乐着。 讲了一上午，我觉得老师这个活挺累。 非常想听到互动，而不是干干巴巴的讲，这让我回想起来我的学习生涯，我一直都是课堂上的活跃分子，老师讲的每个点，我都会大声说：对。有时候搞的同学们都哈哈大笑。 后面还有许多事需要再考虑，希望团队里每个成员都能快速成长，能找到好工作，这就是我的想法。","tags":[]},{"title":"新的征途","date":"2017-02-12T13:01:31.000Z","path":"2017/02/12/新的征途/","text":"2017年开学早早的就来了，路过了北京，见了些朋友，还都不错，过着自己的生活，并没有太多变化。很高兴的是，自己搭建了一个hexo博客，也不知道为什么。之前大学一直用csdn，而如今也转向自己的博客。2017一切都是新的，一切都要努力的去前行。祝好。","tags":[]}]